{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import zlib\n",
    "from pprint import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrackedWord:\n",
    "    def __init__(self, word, parent):\n",
    "        self.word = word\n",
    "        self.nearby = {}\n",
    "        self.count = 1\n",
    "        self.weight = None\n",
    "        self.parent = parent\n",
    "    def __iadd__(self, other):\n",
    "        self.count += other\n",
    "        return self\n",
    "    def addNearbyWord(self, word, N):\n",
    "        \"\"\"Add word to the nearby map as appearing N away.\"\"\"\n",
    "        if N not in self.nearby:\n",
    "            self.nearby[N] = Counter()\n",
    "        if word != '':\n",
    "            self.nearby[N][word] += 1\n",
    "    def getNearbyWords(self, N):\n",
    "        \"\"\"Return all words which appeared N away from self.\n",
    "        \n",
    "        Args:\n",
    "            N: Integer representing far away a word must be to be considered nearby\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of all words which appeared N away.\n",
    "        \"\"\"\n",
    "        if N in self.nearby:\n",
    "            return dict(self.nearby[N])\n",
    "        else:\n",
    "            return dict()\n",
    "    def getNearbyWordsInRange(self, *args):\n",
    "        \"\"\"Return all words which which appear 1 to N words away from self.\n",
    "        \n",
    "        Args:\n",
    "            args: Zero, One, or Two arguments representing the lower and upper\n",
    "                  bounds of the range to be found. If only one argument is provided,\n",
    "                  it will be an upper bound.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing all found words and their number of occurrences.\n",
    "        \"\"\"\n",
    "        lower = 1\n",
    "        upper = 1\n",
    "        if len(args) < 1:\n",
    "            return dict()\n",
    "        if len(args) == 1:\n",
    "            upper = args[0]\n",
    "        if len(args) == 2:\n",
    "            lower = args[0]\n",
    "            upper = args[1]\n",
    "        cumulative = Counter()\n",
    "        for i in range(lower, upper+1):\n",
    "            cumulative += Counter(self.getNearbyWords(i))\n",
    "        return dict(cumulative)\n",
    "    def getWeight(self):\n",
    "        \"\"\"Return the TF-IDF value for a word. \n",
    "        \n",
    "        The TF-IDF is ther term frequency/inverse document frequency of a word. \n",
    "        This represents the importance of the word relative to the work and the\n",
    "        other works in the parent model.\n",
    "        \"\"\"\n",
    "        if self.weight is None:\n",
    "            df = float(self.parent.getNumberOfDocumentsContaining(self.word))\n",
    "            n = float(self.parent.getNumberOfDocuments())\n",
    "            self.weight = float(self.count) * math.log(n/df)\n",
    "        return self.weight\n",
    "    \n",
    "    def getCount(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrackedWordModel:\n",
    "    def __init__(self, text, parent, meta={}):\n",
    "        self.text = zlib.compress(text)\n",
    "        self.words = {}\n",
    "        self.trackedDistances = set([0])\n",
    "        self.meta = meta\n",
    "        self.parent = parent\n",
    "\n",
    "    def getText(self):\n",
    "        return zlib.decompress(self.text)\n",
    "    \n",
    "    def setupTracking(self, N):\n",
    "        \"\"\"Update the word stracking model to track up to N words away if not already tracked.\n",
    "        \n",
    "        Args: \n",
    "            N: Inclusive maximum distance for words to track.\n",
    "            \n",
    "        Returns:\n",
    "            Self, for chaining.\n",
    "        \"\"\"\n",
    "        words = self.getText().split()\n",
    "        if N in self.trackedDistances:\n",
    "            return self\n",
    "        for i, word in enumerate(words):\n",
    "            if word not in self.words:\n",
    "                self.words[word] = TrackedWord(word, self)\n",
    "            for j in range(N, 0, -1):\n",
    "                if j in self.trackedDistances:\n",
    "                    break\n",
    "                wNback = words[i-j] if i-j >= 0 else ''\n",
    "                wNfor = words[i+j] if i+j < len(words) else ''\n",
    "                self.words[word].addNearbyWord(wNback, j)\n",
    "                self.words[word].addNearbyWord(wNfor, j)\n",
    "                self.words[word] += 1\n",
    "        for i in range(N, 0, -1):\n",
    "            if i not in self.trackedDistances:\n",
    "                self.trackedDistances.add(i)\n",
    "            else:\n",
    "                break\n",
    "        return self\n",
    "                \n",
    "    def getWordsNear(self, word, N):\n",
    "        \"\"\"Return a dictionary of words which are exactly N words away from word\n",
    "        \n",
    "        Args:\n",
    "            word: The string being searched for.\n",
    "            N: The distance away from word being searched.\n",
    "        Return:\n",
    "            Dictionary containing all nearby words, or an empty dictionary if word not found.\n",
    "        \"\"\"\n",
    "        if N not in self.trackedDistances:\n",
    "            self.setupTracking(N)\n",
    "        if word in self.words:\n",
    "            return dict(self.words[word].getNearbyWords(N))\n",
    "        else:\n",
    "            return dict()\n",
    "        \n",
    "    def getNearbyWordsInRange(self, word, *args):\n",
    "        \"\"\"Return all words which which appear 1 to N words away from self.\n",
    "        \n",
    "        Args:\n",
    "            args: Zero, One, or Two arguments representing the lower and upper\n",
    "                  bounds of the range to be found. If only one argument is provided,\n",
    "                  it will be an upper bound.\n",
    "            word: The word being searched for.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing all found words and their number of occurrences.\n",
    "        \"\"\"\n",
    "        lower = 1\n",
    "        upper = 1\n",
    "        if len(args) < 1:\n",
    "            return dict()\n",
    "        if len(args) == 1:\n",
    "            upper = args[0]\n",
    "        if len(args) == 2:\n",
    "            lower = args[0]\n",
    "            upper = args[1]\n",
    "            \n",
    "        if upper not in self.trackedDistances:\n",
    "            self.setupTracking(upper)\n",
    "        if word in self.words:\n",
    "            nearby = self.words[word].getNearbyWordsInRange(lower, upper)\n",
    "            return nearby\n",
    "        else:\n",
    "            return dict()\n",
    "        \n",
    "    def getMostFrequent(self):\n",
    "        words = set()\n",
    "        if len(self.words) == 0:\n",
    "            self.setupTracking(1)\n",
    "        for word in self.words:\n",
    "            words.add((self.words[word].getCount(), word))\n",
    "        words = sorted(words, reverse=True)\n",
    "        return words\n",
    "    \n",
    "    def getMostImportant(self):\n",
    "        words = set()\n",
    "        if len(self.words) == 0:\n",
    "            self.setupTracking(1)\n",
    "        for word in self.words:\n",
    "            words.add((self.words[word].getWeight(), word))\n",
    "        words = sorted(words, reverse=True)\n",
    "        return words\n",
    "    \n",
    "    def getNumberOfDocuments(self):\n",
    "        return self.parent.getNumberOfDocuments()\n",
    "    \n",
    "    def getNumberOfDocumentsContaining(self, word):\n",
    "        return self.parent.getNumberOfDocumentsContaining(word)\n",
    "    \n",
    "    def getWord(self, word):\n",
    "        if len(self.words) == 0:\n",
    "            self.setupTracking(1)\n",
    "        if word in self.words:\n",
    "            return self.words[word]\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def getWords(self):\n",
    "        if len(self.words) == 0:\n",
    "            self.setupTracking(1)\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordModelCollection:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.documentFrequencyCounter = Counter()\n",
    "    def updateModel(self, text, meta):\n",
    "        '''Add or update a model in the collection.\n",
    "        \n",
    "        If the title isn't in the collection, add it and update the DF count\n",
    "        If the title is in the collection but the text is identical, do nothing\n",
    "        If the title is in the collection but the text isn't identical, remove\n",
    "        the set of words from the counter and add the set of the new text\n",
    "        '''\n",
    "        title = meta['title'].lower()\n",
    "        if title in self.models:\n",
    "            if self.models[title].getText() == text:\n",
    "                return False\n",
    "            else:\n",
    "                self.documentFrequencyCounter -= set(self.models[title].getText().split())\n",
    "        self.models[title] = TrackedWordModel(text, self, meta)\n",
    "        self.updateDocumentFrequency(text)\n",
    "        return True\n",
    "    \n",
    "    def getModel(self, title):\n",
    "        return self.models[title.lower()]\n",
    "    def getModels(self):\n",
    "        return self.models\n",
    "    def getNumberOfDocuments(self):\n",
    "        return len(self.models)\n",
    "    def getNumberOfDocumentsContaining(self, word):\n",
    "        return self.documentFrequencyCounter[word]\n",
    "    def updateDocumentFrequency(self, text=''):\n",
    "        if text:\n",
    "            self.documentFrequencyCounter.update(set(text.split()))\n",
    "        self.documentFrequencyCounter = Counter()\n",
    "        for model in self.models.values():\n",
    "            self.documentFrequencyCounter.update(set(model.getText().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Amy': 1,\n",
       "         'Ryan': 1,\n",
       "         'also': 1,\n",
       "         \"doesn't\": 1,\n",
       "         'going': 1,\n",
       "         'is': 1,\n",
       "         'like': 1,\n",
       "         'playground': 2,\n",
       "         'the': 2,\n",
       "         'to': 1})"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = Counter(\"Bill is going to the playground\".split())\n",
    "c2 = Counter(\"Amy is also going to the playground\".split())\n",
    "c3 = Counter(\"Ryan doesn't like the playground\".split())\n",
    "c1+c2-c1+c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadFile(f):\n",
    "    lines = []\n",
    "    text = []\n",
    "    with open(f, mode='r') as infile:\n",
    "        lines = infile.readlines()\n",
    "    started = False\n",
    "    ended = False\n",
    "    for line in lines:\n",
    "        if not started:\n",
    "            if '*** START' in line or '***START' in line:\n",
    "                started = True\n",
    "            continue\n",
    "        if '*** END' in line or '***END' in line:\n",
    "            break\n",
    "        line = line.strip('\\n')\n",
    "        line = unicode(line, \"ascii\", errors=\"ignore\")\n",
    "        line =  re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "        line = line.lower()\n",
    "        text.extend(line.split())\n",
    "    text = \" \".join([w for w in text if suitableWord(w)])\n",
    "    return text\n",
    "\n",
    "def suitableWord(word):\n",
    "    if len(word) < 2:\n",
    "        return False\n",
    "    if word in stops:\n",
    "        return False\n",
    "    if word not in english:\n",
    "        return False\n",
    "    if word in warriner and warriner[word]['arousal'] < 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def getGutenbergMeta(f):\n",
    "    data = {}\n",
    "    with open(f) as infile:\n",
    "        for line in infile.readlines():\n",
    "            if line.startswith('Title: '):\n",
    "                data['title'] = unicode(line[len('Title: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "            if line.startswith('Author: '):\n",
    "                data['author'] = unicode(line[len('Author: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "    return data\n",
    "\n",
    "def loadModels(maxmodels=-1, debugging=False, warriner=False):\n",
    "    models = []\n",
    "    files = glob.glob('data/*.txt')\n",
    "    maxmodels = maxmodels if maxmodels >0 else len(files)\n",
    "    for i, f in enumerate(files[:maxmodels]):\n",
    "        meta = getGutenbergMeta(f)\n",
    "        if debugging:\n",
    "            print '['+str(i)+'] Currently processing ', meta['title'], '...'\n",
    "        models.append({'text': loadFile(f), 'meta': meta})\n",
    "    return models\n",
    "\n",
    "stops = set(json.load(open('data/nltkstopwords.json', 'r')))\n",
    "english = set(json.load(open('data/english.json', 'r')))\n",
    "with open('models/warriner.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "    warriner = {rows[1]: {'valence': (float)(rows[2]), 'arousal': (float)(rows[5]), 'dominance': (float)(rows[8])} for rows in reader}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collect = WordModelCollection()\n",
    "meta = getGutenbergMeta('data/dracula.txt')\n",
    "text = loadFile('data/dracula.txt')\n",
    "collect.updateModel(text, meta)\n",
    "models = loadModels(maxmodels=5)\n",
    "for model in models:\n",
    "    collect.updateModel(model['text'], model['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    def __init__(self, debugging=False, maxmodels=-1, warriner=False):\n",
    "        self.debugging = debugging\n",
    "        self.models = self.loadModels(maxmodels=maxmodels, warriner=warriner)\n",
    "        \n",
    "    def loadModels(self, maxmodels=-1, warriner=False):\n",
    "        models = {}\n",
    "        WordTrackModel.df = Counter()\n",
    "        WordTrackModel.totalworks = 0\n",
    "        files = glob.glob('data/*.txt')\n",
    "        maxmodels = maxmodels if maxmodels >0 else len(files)\n",
    "        for i, f in enumerate(files[:maxmodels]):\n",
    "            data = self.getGutenbergMeta(f)\n",
    "            if self.debugging:\n",
    "                print '['+str(i)+'] Currently processing ', data, '...'\n",
    "            models[data['title']] = WordTrackModel(data['title'], f, data, warriner)\n",
    "            models[data['title']].distTrack(3)\n",
    "        return models\n",
    "    def getGutenbergMeta(self, f):\n",
    "        data = {'title': 'xxx', 'author': 'xxx'}\n",
    "        with open(f) as infile:\n",
    "            for line in infile.readlines():\n",
    "                if line.startswith('Title: '):\n",
    "                    data['title'] = unicode(line[len('Title: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "                if line.startswith('Author: '):\n",
    "                    data['author'] = unicode(line[len('Author: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "        return data\n",
    "    def search_clusters(self, search, models=None, dist_away=2, orderby='importance', limit=20, exclusive=True):\n",
    "        if not models:\n",
    "            models = [m[1] for m in self.models.items()]\n",
    "        near = dist_away\n",
    "        clusters = {}\n",
    "        similarclusters = []\n",
    "        for model in models:\n",
    "            clusters[model.name] = set([w[0] for w in model.sumNear(search, near, limit, orderby=orderby)])\n",
    "            similarclusters = []\n",
    "            for cluster1 in clusters:\n",
    "                for cluster2 in clusters:\n",
    "                    if cluster1 != cluster2:\n",
    "                        if len(clusters[cluster1] & clusters[cluster2]) > 2:\n",
    "                            newclust = ((cluster1, clusters[cluster1]), (cluster2, clusters[cluster2]))\n",
    "                            if (newclust[1], newclust[0]) not in similarclusters:\n",
    "                                similarclusters.append(newclust)\n",
    "        if not exclusive:\n",
    "            return similarclusters\n",
    "        else:\n",
    "            return [((cluster[0][0], cluster[1][0]), cluster[0][1] & cluster[1][1] )for cluster in similarclusters]\n",
    "    \n",
    "    def most_similar_to(self, search, limit=10):\n",
    "        models = self.models\n",
    "        if isinstance(search, str): \n",
    "            search = [search]\n",
    "        similars = {}\n",
    "        for model in models:\n",
    "            similars[model] = Counter()\n",
    "            numwords = float(len(models[model].wordlist()))\n",
    "            for word in search:\n",
    "                if word in models[model].words:\n",
    "                    similars[model].update({word: math.log(1+models[model].words[word].count/numwords)})\n",
    "        sortedsums = sorted([(sum(similars[w].values()), w, similars[w].items()) for w in similars], reverse=True)[:limit]\n",
    "        return sortedsums\n",
    "    \n",
    "    def common_clusters(self, models, dist_away=2, orderby='importance', limit=20, exclusive=True):\n",
    "        models = [m[1] for m in models]\n",
    "        n = len(models)\n",
    "        wordsets = [set(m.words.keys()) for m in models]\n",
    "        commonwords = set.intersection(*wordsets)\n",
    "        commonclusters = {}\n",
    "        for word in commonwords:\n",
    "            found = kb.search_clusters(word, models, dist_away, orderby, limit, exclusive)\n",
    "            if len(found) > 0:\n",
    "                if word not in commonclusters:\n",
    "                    commonclusters[word] = []\n",
    "                commonclusters[word].extend(found)\n",
    "\n",
    "        # If there is n choose 2 entries, then all of them have commonalities with each other\n",
    "        commonclusters = {word: commonclusters[word] for word in commonclusters if len(commonclusters[word]) >= n * (n-1) / 2}\n",
    "        return commonclusters\n",
    "    \n",
    "    def common_cluster_words(self, models, dist_away=2, orderby='importance', limit=20, exclusive=True, join='union'):\n",
    "        commonclusters = self.common_clusters(models, dist_away,orderby,limit,exclusive)\n",
    "        wordsets = {}\n",
    "        for word in commonclusters:\n",
    "            wordsets[word] = commonclusters[word][0][1]\n",
    "            for l in commonclusters[word]:\n",
    "                if join == 'union':\n",
    "                    wordsets[word] = wordsets[word] | l[1]\n",
    "                if join == 'intersection':\n",
    "                    wordsets[word] = wordsets[word] & l[1]\n",
    "        return wordsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing On: Filtering\n",
    "\n",
    "The results I'm getting now are definitely interesting and hopefully useable. However the problem remains that many of the words are blande and pointless. \n",
    "\n",
    "### Tried\n",
    "\n",
    "* Removing all words not in Warriner: Way too many good words get removed.\n",
    "* Removing all words found in Warriner with arousal < 4: Good results! A few decent words cut, but many, many awful words as well. Also a tunable param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kbw = KnowledgeBase(maxmodels=5, warriner=True)\n",
    "kb = KnowledgeBase(maxmodels=5, warriner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = set([m[1] for m in kbw.models['The Return of Sherlock Holmes'].most_important()])\n",
    "s2 = set([m[1] for m in kb.models['The Return of Sherlock Holmes'].most_important()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer Deeper\n",
    "\n",
    "Looking into how to go \"one layer deeper\"\n",
    "\n",
    "* Find the common cluster words between two models\n",
    "* For each word, examine their sumNear(N) and build a new set\n",
    "* Return the intersection of those sets\n",
    "\n",
    "This allows you to find words which, while not *directly* related, have a bit of a triangular dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'The Life and Adventures of Robinson Crusoe', <__main__.WordTrackModel instance at 0x7f24ba663ef0>) (u'The Iliad of Homer', <__main__.WordTrackModel instance at 0x7f24b8cfbb48>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ago': {'long', 'may', 'one'},\n",
       " 'aloud': {'god', 'thou', 'ye'},\n",
       " 'ask': {'god', 'hast', 'thou'},\n",
       " 'east': {'shore', 'south', 'west'},\n",
       " 'float': {'high', 'one', 'shore'},\n",
       " 'west': {'east', 'north', 'sea'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print kb.models.items()[0], kb.models.items()[1]\n",
    "kb.common_cluster_words([kb.models.items()[0], kb.models.items()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boat',\n",
       " 'came',\n",
       " 'could',\n",
       " 'first',\n",
       " 'go',\n",
       " 'great',\n",
       " 'island',\n",
       " 'land',\n",
       " 'little',\n",
       " 'made',\n",
       " 'might',\n",
       " 'ship',\n",
       " 'shore',\n",
       " 'towards',\n",
       " 'two',\n",
       " 'upon',\n",
       " 'way',\n",
       " 'went',\n",
       " 'would'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set([m[0] for m in kb.models.items()[0][1].words['sea'].sumNear(3).most_common()[:30]])\n",
    "s2 = set([m[0] for m in kb.models.items()[0][1].words['shore'].sumNear(3).most_common()[:30]])\n",
    "s1&s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts moving forward\n",
    "\n",
    "Must clear out poor words. They're really bad atm. Use Warriner to remove any found, bottom ones and potentially POS tagging to remove words tagged with a certain tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = [1,2,3,4,5]\n",
    "f[:len(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 3: 2, 9: 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(Counter([1,2,3,3,2,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(json.load(open('data/nltkstopwords.json', 'r')))\n",
    "english = set(json.load(open('data/english.json', 'r')))\n",
    "with open('models/warriner.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "    warriner = {rows[1]: {'valence': (float)(rows[2]), 'arousal': (float)(rows[5]), 'dominance': (float)(rows[8])} for rows in reader}\n",
    "    \n",
    "  \n",
    "    def loadfile(self, f):\n",
    "        lines = []\n",
    "        text = []\n",
    "        with open(f, mode='r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        started = False\n",
    "        ended = False\n",
    "        for line in lines:\n",
    "            if not started:\n",
    "                if '*** START' in line or '***START' in line:\n",
    "                    started = True\n",
    "                continue\n",
    "            if '*** END' in line or '***END' in line:\n",
    "                break\n",
    "            line = line.strip('\\n')\n",
    "            line = unicode(line, \"ascii\", errors=\"ignore\")\n",
    "            line =  re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "            line = line.lower()\n",
    "            text.extend(line.split())\n",
    "        text = \" \".join([w for w in text if self.suitableWord(w)])\n",
    "        self.df.update(set(text.split()))\n",
    "        return zlib.compress(text)\n",
    "    \n",
    "    def suitableWord(self, word):\n",
    "        if word in self.stops:\n",
    "            return False\n",
    "        if word not in self.english:\n",
    "            return False\n",
    "        if self.filterwarriner:\n",
    "            if word in WordTrackModel.warriner and WordTrackModel.warriner[word]['arousal'] < 4:\n",
    "                return False\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
