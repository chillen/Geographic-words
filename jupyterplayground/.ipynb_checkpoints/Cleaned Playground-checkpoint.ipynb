{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import zlib\n",
    "from pprint import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrackedWord:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.nearby = {}\n",
    "        self.count = 1\n",
    "        self.weight = -1\n",
    "    def __iadd__(self, other):\n",
    "        self.count += other\n",
    "        return self\n",
    "    def addNear(self, N, word1):\n",
    "        if N not in self.nearby:\n",
    "            self.nearby[N] = Counter()\n",
    "        if word1 != '':\n",
    "            self.nearby[N][word1] += 1\n",
    "    def near(self, N):\n",
    "        '''Returns the words which are N away'''\n",
    "        if N in self.nearby:\n",
    "            return self.nearby[N]\n",
    "        else:\n",
    "            return Counter()\n",
    "    def sumNear(self, N):\n",
    "        '''Returns the sum of all words within the range 1-N'''\n",
    "        if N in self.nearby:\n",
    "            relevant = []\n",
    "            for i in range(1, N+1):\n",
    "                relevant.append(self.nearby[i])\n",
    "            return reduce((lambda x, y: x + y), relevant)\n",
    "    def getWeight(self):\n",
    "        '''Returns tf-idf of the word'''\n",
    "        if self.weight < 0:\n",
    "            df = float(WordTrackModel.df[self.word])\n",
    "            n = float(WordTrackModel.totalworks)\n",
    "            self.weight = float(self.count) * math.log(n/df)\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WordTrackModel:\n",
    "    \n",
    "    stops = set(json.load(open('data/nltkstopwords.json', 'r')))\n",
    "    english = set(json.load(open('data/english.json', 'r')))\n",
    "    totalworks = 0\n",
    "    df = Counter()\n",
    "    warriner = None\n",
    "\n",
    "    with open('models/warriner.csv', mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        next(reader)\n",
    "        warriner = {rows[1]: {'valence': (float)(rows[2]), 'arousal': (float)(rows[5]), 'dominance': (float)(rows[8])} for rows in reader}\n",
    "    \n",
    "    def __init__(self, name, f, data={}, filterwarriner=False):\n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.text = self.loadfile(f)\n",
    "        self.words = {}\n",
    "        self.tracked = set([0])\n",
    "        self.data = data\n",
    "        self.filterwarriner = warriner\n",
    "        WordTrackModel.totalworks += 1\n",
    "    \n",
    "    def loadfile(self, f):\n",
    "        lines = []\n",
    "        text = []\n",
    "        with open(f, mode='r') as infile:\n",
    "            lines = infile.readlines()\n",
    "        started = False\n",
    "        ended = False\n",
    "        for line in lines:\n",
    "            if not started:\n",
    "                if '*** START' in line or '***START' in line:\n",
    "                    started = True\n",
    "                continue\n",
    "            if '*** END' in line or '***END' in line:\n",
    "                break\n",
    "            line = line.strip('\\n')\n",
    "            line = unicode(line, \"ascii\", errors=\"ignore\")\n",
    "            line =  re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "            line = line.lower()\n",
    "            text.extend(line.split())\n",
    "        text = \" \".join([w for w in text if self.suitableWord(w)])\n",
    "        self.df.update(set(text.split()))\n",
    "        return zlib.compress(text)\n",
    "    \n",
    "    def suitableWord(self, word):\n",
    "        if word in self.stops:\n",
    "            return False\n",
    "        if word not in self.english:\n",
    "            return False\n",
    "        if self.filterwarriner:\n",
    "            if word not in WordTrackModel.warriner:\n",
    "                return False\n",
    "            if WordTrackModel.warriner[word]['arousal'] < 4:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def wordlist(self):\n",
    "        return zlib.decompress(self.text).split()\n",
    "    \n",
    "    def distTrack(self, N):\n",
    "        words = self.wordlist()\n",
    "        if N in self.tracked:\n",
    "            return self\n",
    "        for i, word in enumerate(words):\n",
    "            if word not in self.words:\n",
    "                self.words[word] = TrackedWord(word)\n",
    "            for j in range(N, 0, -1):\n",
    "                if j in self.tracked:\n",
    "                    break\n",
    "                wNback = words[i-j] if i-j >= 0 else ''\n",
    "                wNfor = words[i+j] if i+j < len(words) else ''\n",
    "                self.words[word].addNear(j, wNback)\n",
    "                self.words[word].addNear(j, wNfor)\n",
    "                self.words[word] += 1\n",
    "        for i in range(N, 0, -1):\n",
    "            if i not in self.tracked:\n",
    "                self.tracked.add(i)\n",
    "            else:\n",
    "                break\n",
    "        return self\n",
    "                \n",
    "    def near(self, word, N, most=0):\n",
    "        if N not in self.tracked:\n",
    "            self.distTrack(N)\n",
    "        if word in self.words:\n",
    "            return self.words[word].near(N) if most <= 0 else self.words[word].near(N).most_common(most)\n",
    "        else:\n",
    "            return Counter() if most <= 0 else []\n",
    "        \n",
    "    def orderby(self, words, method, reverse=True):\n",
    "        if method == 'frequency':\n",
    "            return words.most_common()\n",
    "        if method == 'importance':\n",
    "            sortedwords = sorted([ (self.words[w].getWeight(), w) for w in words], reverse=reverse)\n",
    "            return [(w[1], w[0]) for w in sortedwords]\n",
    "        \n",
    "    def sumNear(self, word, N, most=0, orderby='frequency'):\n",
    "        if N not in self.tracked:\n",
    "            self.distTrack(N)\n",
    "        if word in self.words:\n",
    "            nearby = self.words[word].sumNear(N)\n",
    "            nearby = self.orderby(nearby, orderby)\n",
    "            return nearby if most == 0 else nearby[:most]\n",
    "        else:\n",
    "            return []\n",
    "    def most_common(self, N=-1):\n",
    "        if N > 0: return Counter(self.wordlist()).most_common(N)\n",
    "        return Counter(self.wordlist()).most_common()\n",
    "    def most_important(self, maxnum=-1):\n",
    "        words = set()\n",
    "        for word in self.words:\n",
    "            words.add((self.words[word].getWeight(), word))\n",
    "        if maxnum > 0: return sorted(words, reverse=True)[:maxnum]\n",
    "        return sorted(words, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    def __init__(self, debugging=False, maxmodels=-1, warriner=False):\n",
    "        self.debugging = debugging\n",
    "        self.models = self.loadModels(maxmodels=maxmodels, warriner=warriner)\n",
    "        \n",
    "    def loadModels(self, maxmodels=-1, warriner=False):\n",
    "        models = {}\n",
    "        WordTrackModel.df = Counter()\n",
    "        WordTrackModel.totalworks = 0\n",
    "        files = glob.glob('data/*.txt')\n",
    "        maxmodels = maxmodels if maxmodels >0 else len(files)\n",
    "        for i, f in enumerate(files[:maxmodels]):\n",
    "            data = self.getGutenbergMeta(f)\n",
    "            if self.debugging:\n",
    "                print '['+str(i)+'] Currently processing ', data, '...'\n",
    "            models[data['title']] = WordTrackModel(data['title'], f, data, warriner)\n",
    "            models[data['title']].distTrack(3)\n",
    "        return models\n",
    "    def getGutenbergMeta(self, f):\n",
    "        data = {'title': 'xxx', 'author': 'xxx'}\n",
    "        with open(f) as infile:\n",
    "            for line in infile.readlines():\n",
    "                if line.startswith('Title: '):\n",
    "                    data['title'] = unicode(line[len('Title: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "                if line.startswith('Author: '):\n",
    "                    data['author'] = unicode(line[len('Author: '):].strip('\\n'), \"ascii\", errors=\"ignore\")\n",
    "        return data\n",
    "    def search_clusters(self, search, models=None, dist_away=2, orderby='importance', limit=20, exclusive=True):\n",
    "        if not models:\n",
    "            models = [m[1] for m in self.models.items()]\n",
    "        near = dist_away\n",
    "        clusters = {}\n",
    "        similarclusters = []\n",
    "        for model in models:\n",
    "            clusters[model.name] = set([w[0] for w in model.sumNear(search, near, limit, orderby=orderby)])\n",
    "            similarclusters = []\n",
    "            for cluster1 in clusters:\n",
    "                for cluster2 in clusters:\n",
    "                    if cluster1 != cluster2:\n",
    "                        if len(clusters[cluster1] & clusters[cluster2]) > 2:\n",
    "                            newclust = ((cluster1, clusters[cluster1]), (cluster2, clusters[cluster2]))\n",
    "                            if (newclust[1], newclust[0]) not in similarclusters:\n",
    "                                similarclusters.append(newclust)\n",
    "        if not exclusive:\n",
    "            return similarclusters\n",
    "        else:\n",
    "            return [((cluster[0][0], cluster[1][0]), cluster[0][1] & cluster[1][1] )for cluster in similarclusters]\n",
    "    \n",
    "    def most_similar_to(self, search, limit=10):\n",
    "        models = self.models\n",
    "        if isinstance(search, str): \n",
    "            search = [search]\n",
    "        similars = {}\n",
    "        for model in models:\n",
    "            similars[model] = Counter()\n",
    "            numwords = float(len(models[model].wordlist()))\n",
    "            for word in search:\n",
    "                if word in models[model].words:\n",
    "                    similars[model].update({word: math.log(1+models[model].words[word].count/numwords)})\n",
    "        sortedsums = sorted([(sum(similars[w].values()), w, similars[w].items()) for w in similars], reverse=True)[:limit]\n",
    "        return sortedsums\n",
    "    \n",
    "    def common_clusters(self, models, dist_away=2, orderby='importance', limit=20, exclusive=True):\n",
    "        models = [m[1] for m in models]\n",
    "        n = len(models)\n",
    "        wordsets = [set(m.words.keys()) for m in models]\n",
    "        commonwords = set.intersection(*wordsets)\n",
    "        commonclusters = {}\n",
    "        for word in commonwords:\n",
    "            found = kb.search_clusters(word, models, dist_away, orderby, limit, exclusive)\n",
    "            if len(found) > 0:\n",
    "                if word not in commonclusters:\n",
    "                    commonclusters[word] = []\n",
    "                commonclusters[word].extend(found)\n",
    "\n",
    "        # If there is n choose 2 entries, then all of them have commonalities with each other\n",
    "        commonclusters = {word: commonclusters[word] for word in commonclusters if len(commonclusters[word]) >= n * (n-1) / 2}\n",
    "        return commonclusters\n",
    "    \n",
    "    def common_cluster_words(self, models, dist_away=2, orderby='importance', limit=20, exclusive=True, join='union'):\n",
    "        commonclusters = self.common_clusters(models, dist_away,orderby,limit,exclusive)\n",
    "        wordsets = {}\n",
    "        for word in commonclusters:\n",
    "            wordsets[word] = commonclusters[word][0][1]\n",
    "            for l in commonclusters[word]:\n",
    "                if join == 'union':\n",
    "                    wordsets[word] = wordsets[word] | l[1]\n",
    "                if join == 'intersection':\n",
    "                    wordsets[word] = wordsets[word] & l[1]\n",
    "        return wordsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing On\n",
    "\n",
    "The results I'm getting now are definitely interesting and hopefully useable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "WordTrackModel instance has no attribute 'filterwarriner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e052de5b252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkbw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnowledgeBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKnowledgeBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ea8577888c95>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, debugging, maxmodels, warriner)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarriner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloadModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ea8577888c95>\u001b[0m in \u001b[0;36mloadModels\u001b[0;34m(self, maxmodels, warriner)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m'['\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'] Currently processing '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordTrackModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarriner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistTrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f6a69e02cadb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, f, data, filterwarriner)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f6a69e02cadb>\u001b[0m in \u001b[0;36mloadfile\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuitableWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f6a69e02cadb>\u001b[0m in \u001b[0;36msuitableWord\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarriner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWordTrackModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarriner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: WordTrackModel instance has no attribute 'filterwarriner'"
     ]
    }
   ],
   "source": [
    "kbw = KnowledgeBase(maxmodels=5, warriner=True)\n",
    "kb = KnowledgeBase(maxmodels=5, warriner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(kbw.models['The Return of Sherlock Holmes'].most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(kb.models['The Return of Sherlock Holmes'].most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer Deeper\n",
    "\n",
    "Looking into how to go \"one layer deeper\"\n",
    "\n",
    "* Find the common cluster words between two models\n",
    "* For each word, examine their sumNear(N) and build a new set\n",
    "* Return the intersection of those sets\n",
    "\n",
    "This allows you to find words which, while not *directly* related, have a bit of a triangular dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'The Life and Adventures of Robinson Crusoe', <__main__.WordTrackModel instance at 0x7f24ba663ef0>) (u'The Iliad of Homer', <__main__.WordTrackModel instance at 0x7f24b8cfbb48>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ago': {'long', 'may', 'one'},\n",
       " 'aloud': {'god', 'thou', 'ye'},\n",
       " 'ask': {'god', 'hast', 'thou'},\n",
       " 'east': {'shore', 'south', 'west'},\n",
       " 'float': {'high', 'one', 'shore'},\n",
       " 'west': {'east', 'north', 'sea'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print kb.models.items()[0], kb.models.items()[1]\n",
    "kb.common_cluster_words([kb.models.items()[0], kb.models.items()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boat',\n",
       " 'came',\n",
       " 'could',\n",
       " 'first',\n",
       " 'go',\n",
       " 'great',\n",
       " 'island',\n",
       " 'land',\n",
       " 'little',\n",
       " 'made',\n",
       " 'might',\n",
       " 'ship',\n",
       " 'shore',\n",
       " 'towards',\n",
       " 'two',\n",
       " 'upon',\n",
       " 'way',\n",
       " 'went',\n",
       " 'would'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set([m[0] for m in kb.models.items()[0][1].words['sea'].sumNear(3).most_common()[:30]])\n",
    "s2 = set([m[0] for m in kb.models.items()[0][1].words['shore'].sumNear(3).most_common()[:30]])\n",
    "s1&s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts moving forward\n",
    "\n",
    "Must clear out poor words. They're really bad atm. Use Warriner to remove any found, bottom ones and potentially POS tagging to remove words tagged with a certain tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = [1,2,3,4,5]\n",
    "f[:len(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
